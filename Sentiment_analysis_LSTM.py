# -*- coding: utf-8 -*-
"""SentimentAnalysisLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eoHICOz-AgssYX-5haTSNIj45qfZYwdS
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import re
import nltk

"""### Reading data
The filename of the given dataset "Dataset-Sentiment Analysis (Task-ML Engineer).xlsx"
info() function to get the information of the dataset
The plot() function is used to draw points (markers) in a diagram
"""

# read by default sheet of an excel file
dataframe1 = pd.read_excel('Dataset-Sentiment Analysis.xlsx')

print(dataframe1)

dataframe1.info() # to get the information of the dataset

dataframe1['sentiment'].value_counts().plot(kind="bar", rot=0)  #counts of multiclass sentiment values through a bar diagram

#assinging numerical values to the multiclass categories of sentiment
classDict = {'negative': 0, 'positive': 1,'neutral': 2}
dataframe2=dataframe1.replace({"sentiment": classDict})
dataframe2

"""## Bert Embeddings"""

from transformers import BertModel, BertTokenizer
from normalizer import normalize
import torch

model = BertModel.from_pretrained("bert-base-multilingual-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-uncased")

# Create an empty list to store the BERT embeddings
bert_embeddings = []

# Iterate through each row in the DataFrame
for index, row in dataframe2.iterrows():
    # Concatenate the 'conversation_text' columns
    text = row['conversation_text']

    # Tokenize the text and convert it to BERT embeddings
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # Average the embeddings across all tokens
        bert_embeddings.append(embeddings)
# Convert the list of BERT embeddings to a DataFrame
bert_embeddings_df = pd.DataFrame(torch.cat(bert_embeddings).numpy())

# Concatenate the BERT embeddings DataFrame with the original DataFrame
result_df = pd.concat([dataframe2, bert_embeddings_df], axis=1)

result_df

bert_embeddings_df

"""## Spliting Dataset (Train=70%, Test= 15%, Val=15%)"""

X=bert_embeddings_df
y=result_df["sentiment"]

from sklearn.model_selection import train_test_split
X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)

"""## Training dataset"""

X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
X_train_tensor

X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
X_test_tensor

y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_train_tensor

y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)
y_test_tensor

X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)
X_val_tensor

y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)
y_val_tensor

X_train_tensor = X_train_tensor.unsqueeze(1)  # Adds a sequence length dimension
X_val_tensor = X_val_tensor.unsqueeze(1)
X_test_tensor = X_test_tensor.unsqueeze(1)

X_train

"""## LSTM

"""

import torch
import torch.nn as nn
import torch.optim as optim

# Define the LSTM Model
class LSTMSentimentClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):
        super(LSTMSentimentClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        h_0 = torch.zeros(2, x.size(0), hidden_size).to(x.device)  # Initialize hidden state
        c_0 = torch.zeros(2, x.size(0), hidden_size).to(x.device)  # Initialize cell state
        out, _ = self.lstm(x, (h_0, c_0))  # LSTM forward pass
        out = out[:, -1, :]  # Get the last time step's output
        out = self.fc(out)  # Fully connected layer
        out = self.softmax(out)  # Softmax for class probabilities
        return out

# Model Parameters
input_size = 768  # BERT embedding size
hidden_size = 128
output_size = 3  # Number of classes (neutral, positive, negative)
num_layers = 2
dropout = 0.2

# Instantiate the model
model = LSTMSentimentClassifier(input_size, hidden_size, output_size, num_layers, dropout)

# Loss and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Example Training Loop
def train_model(model, X_train, y_train, X_valid, y_valid, epochs=5):
    model.train()

    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_valid)
            val_loss = criterion(val_outputs, y_valid)

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}, Validation Loss: {val_loss.item()}")
        model.train()
    return model

trained_model=train_model(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, epochs=50)

"""## Confusion_matrix"""

predictions= trained_model(X_val_tensor)

predictions

# Convert predictions to class labels using argmax
class_labels = np.argmax(predictions.detach().numpy(), axis=1)

print("Predicted Class Labels:")
print(class_labels)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
conf_matrix =confusion_matrix(y_val, class_labels)
print("Confusion Matrix:")
print(conf_matrix)

import matplotlib as plt
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,
                              display_labels=["negetive","positive","neutral"])
disp.plot()

"""## F1 Score"""

# Number of classes
num_classes = conf_matrix.shape[0]

# Calculate Precision, Recall, and F1 Score for each class
f1_scores = []

for i in range(num_classes):
    # True Positives (TP): Diagonal elements
    TP = conf_matrix[i, i]

    # False Positives (FP): Sum of column elements except the diagonal element
    FP = np.sum(conf_matrix[:, i]) - TP

    # False Negatives (FN): Sum of row elements except the diagonal element
    FN = np.sum(conf_matrix[i, :]) - TP

    # Precision and Recall
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0

    # F1 Score
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    f1_scores.append(f1_score)

# Convert to a NumPy array for convenience
f1_scores = np.array(f1_scores)

# Print F1 Scores
print("F1 Scores for each class:")
print(f1_scores)

# Average F1 Scores
f1_score_micro = np.sum(conf_matrix.diagonal()) / np.sum(conf_matrix)
f1_score_macro = np.mean(f1_scores)
f1_score_weighted = np.average(f1_scores, weights=np.sum(conf_matrix, axis=1))

print("Micro Average F1 Score:", f1_score_micro)
print("Macro Average F1 Score:", f1_score_macro)
print("Weighted Average F1 Score:", f1_score_weighted)

